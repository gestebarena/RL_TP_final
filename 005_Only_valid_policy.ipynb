{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb5793f-80b2-4c6b-b15b-51abf33c83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2a7557-b5cd-44e1-984e-5943e1bf5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a9f67b-265e-401e-85c7-55b3827c9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_env import make_reversi_vec_env, SelfPlayEnv\n",
    "import torch as th\n",
    "from players import RandomPlayer, DictPolicyPlayer, GreedyPlayer\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07411d1-263f-445f-9dc8-92ef55cd4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 10\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f7b6a-ab0b-495f-9b9b-03220308f75b",
   "metadata": {},
   "source": [
    "# Modificación de librería para que haga argmax solo sobre las válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65966e76-d302-40f5-be6a-1b00565194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    ActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac109405-906a-4c2e-92bd-0ab6d8146190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([19, 20, 44, 34, 29, 20, 37, 20, 21, 21]), None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8ae4f-2113-4031-8b6a-3b8210285937",
   "metadata": {},
   "source": [
    "# Custom ActorCriticPolicy \n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/policies.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4552377-3076-44dd-a6d4-d504b5915e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b9569a-4fed-4508-8cbd-73b7aac5058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_not_vect = ReversiEnv(board_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561d0e14-d3e3-4c69-b407-d459371848e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "(state, player) = env_not_vect.reset()\n",
    "print(state)\n",
    "print(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58553f62-ab53-41b9-9815-df9706caffc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_not_vect.get_valid((state, player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4898c124-9b43-4088-a366-03adc8b31ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions_mask(state):\n",
    "    player = 1\n",
    "    valid_actions = env_not_vect.get_valid((state, player))\n",
    "    return valid_actions.reshape(-1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d92fe71-689f-4a7f-8f0b-4ee7453a4db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "m = get_actions_mask(env.reset()[0][0])\n",
    "print(m.reshape(board_shape, board_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8599b62-07b5-4972-88fc-126faedeb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args, # Todos los argumentos posicionales de ActorCriticPolicy\n",
    "        actions_mask_func=None, # El nuevo argumento\n",
    "        **kwargs # Todos los argumentos opcionales de ActorCriticPolicy\n",
    "    ):\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "        if actions_mask_func:\n",
    "            self.get_actions_mask = actions_mask_func\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sample_masked_actions(self, obs, distribution, deterministic=False, return_distribution=False):\n",
    "        # Dada las obs y distribuciones luego de evaluar la red neuronal, samplear solo las acciones válidas\n",
    "        # Las obs se usan para que con self.get_actions_mask se obtengan las acciones válidas\n",
    "        # las distribuciones son el resultado de evaluar la red neuronal y van a dar acciones no validas\n",
    "        # Generar una nueva distribución (del lado de los logits preferentemente) donde las acciones no válidas\n",
    "        # tengan probabildad nula de ser muestreadas\n",
    "        # Luego se modifican abajo los métodos\n",
    "        # _predict, forward y evaluate_actions\n",
    "        # Si tiene el flag de return_distribution en true devuelve la distribución nueva\n",
    "        # Caso contrario devuelve las acciones\n",
    "        # Para tener en cuenta, obs tiene dimensión [batch_size, channels, H, W]\n",
    "        # Recomendamos poner un print(obs.shape)\n",
    "        # y correr:\n",
    "        # obs = env.reset()\n",
    "        # actions, _ = model.predict(obs)\n",
    "        # Para sacarse las dudas\n",
    "        \n",
    "        \n",
    "        def get_mask(obs):\n",
    "            masks = np.zeros((len(obs), obs.shape[-1] * obs.shape[-2]))\n",
    "            for i, board in enumerate(obs):\n",
    "                board =  board[0].cpu().numpy()\n",
    "                masks[i] = 1 - self.get_actions_mask(board)\n",
    "            return masks\n",
    " \n",
    "        masks = th.as_tensor(get_mask(obs))\n",
    "        masks[masks == 1] = -np.inf\n",
    "\n",
    "        #print(distribution.logits.dtype, distribution.logits.shape)\n",
    "        #print(masks.dtype, masks.shape)\n",
    "\n",
    "        masked_logits = distribution.logits + masks\n",
    "        #print(masked_logits)\n",
    "\n",
    "        if return_distribution:\n",
    "            return th.distributions.Categorical(logits=masked_logits, validate_args = True)\n",
    "        if deterministic:\n",
    "            return th.argmax(masked_logits, axis=1)\n",
    "        return th.distributions.Categorical(logits=masked_logits).sample()\n",
    "                \n",
    "    \n",
    "    def _predict(self, observation, deterministic=False):\n",
    "        \"\"\"\n",
    "        Get the action according to the policy for a given observation.\n",
    "        :param observation:\n",
    "        :param deterministic: Whether to use stochastic or deterministic actions\n",
    "        :return: Taken action according to the policy\n",
    "        \"\"\"\n",
    "        latent_pi, _, latent_sde = self._get_latent(observation)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n",
    "        \n",
    "        if self.get_actions_mask:\n",
    "            actions = self.sample_masked_actions(observation, distribution.distribution, deterministic=deterministic)\n",
    "        else:\n",
    "            actions = distribution.get_actions(deterministic=deterministic)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def forward(self, obs: th.Tensor, deterministic: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde=latent_sde)\n",
    "        distrib = self.sample_masked_actions(obs, distribution.distribution, return_distribution=True)\n",
    "        \n",
    "        actions = self.sample_masked_actions(obs, distribution.distribution, deterministic=deterministic)\n",
    "\n",
    "        log_prob = distrib.log_prob(actions)        \n",
    "        \n",
    "        #if self.get_actions_mask:\n",
    "        #    actions = self.sample_masked_actions(obs, distribution.distribution, deterministic=deterministic)\n",
    "        #else:\n",
    "        #    actions = distribution.get_actions(deterministic=deterministic)\n",
    "        #\n",
    "        #log_prob = distribution.log_prob(actions)\n",
    "        \n",
    "        \n",
    "        return actions, values, log_prob\n",
    "    \n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor) -> [th.Tensor, th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            and entropy of the action distribution.\n",
    "        \"\"\"\n",
    "        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n",
    "        distrib = self.sample_masked_actions(obs, distribution.distribution, return_distribution=True)\n",
    "\n",
    "        log_prob = distrib.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        return values, log_prob, distrib.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122d6fef-5538-48d5-bffb-e8f92ab55b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    policy_kwargs = {'actions_mask_func': get_actions_mask}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a14de75-e64e-4c2d-b467-1eb57734b925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d500b8c8-ffbf-4175-a803-269502c9e044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Testeo de predict\n",
    "mask = model.policy.get_actions_mask(env.reset()[0][0])\n",
    "print(mask.reshape(board_shape, board_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860228c6-91e8-4a2e-925f-993cc700e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "actions, _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8bf9fbd-5077-4255-8eac-e84462c35378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 44, 34, 37, 43, 20, 29, 20, 26, 26])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar que las acciones son válidas\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09a65995-f994-4f0b-a369-4db7c6981a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([34, 42, 34, 37, 29, 29, 43, 20, 26, 42]),\n",
       " tensor([[ 0.0187],\n",
       "         [ 0.0781],\n",
       "         [ 0.0187],\n",
       "         [-0.0073],\n",
       "         [ 0.0187],\n",
       "         [ 0.0187],\n",
       "         [ 0.0187],\n",
       "         [ 0.0187],\n",
       "         [-0.1200],\n",
       "         [-0.1200]], grad_fn=<AddmmBackward>),\n",
       " tensor([-1.3834, -1.0962, -1.3834, -1.1027, -1.3880, -1.3880, -1.3857, -1.3881,\n",
       "         -1.0998, -1.0984], dtype=torch.float64, grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testeo de forward\n",
    "model.policy(th.from_numpy(obs).to(model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41a057-bfa4-4901-b102-c3d385dedabf",
   "metadata": {},
   "source": [
    "# Corremos PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd2b2d58-5bd1-4963-8fc0-3a7c65590b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 6\n",
    "gamma = 0.99\n",
    "ent_coef = 0.0\n",
    "gae_lambda = 0.95\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b565b606-2942-48a4-af53-5c78608e7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversi_PPO_8by8_0.99_0.95_0.0_10_6_masked_actions_2\n",
      "./models/Reversi_PPO_8by8_0.99_0.95_0.0_10_6_masked_actions_2\n"
     ]
    }
   ],
   "source": [
    "prefix = 'Reversi_PPO'\n",
    "suffix = 'masked_actions_2'\n",
    "model_name = f'{prefix}_{board_shape}by{board_shape}_{gamma}_{gae_lambda}_{ent_coef}_{n_epochs}_{n_envs}_{suffix}'\n",
    "best_model_save_path = f'./models/{model_name}'\n",
    "print(model_name)\n",
    "print(best_model_save_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d8d2c6c-498c-4425-860b-47f4ae943432",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057fb30-8781-4ae3-ae9a-50f0cf814434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8640461a-dd62-4ba1-a8bd-d03509a2789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log='tensorboard_log',\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    ent_coef=ent_coef,\n",
    "    n_epochs=n_epochs,\n",
    "    policy_kwargs = {'actions_mask_func': get_actions_mask}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c554a4a-2b95-45cf-a96c-9c836ca45232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "147117ff-2ff0-4aa8-a7b6-fb1914d54e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El entorno de evaluación no corre en paralelo por eso uno solo\n",
    "eval_env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=1,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80a6e75e-66cd-42d2-a023-ab9f662130b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env = eval_env,\n",
    "    eval_freq=1_000,\n",
    "    n_eval_episodes=500,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=best_model_save_path,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5041b9-700a-473a-9102-6becaad5a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=6640, episode_reward=0.71 +/- 0.70\n",
      "Episode length: 30.06 +/- 0.60\n",
      "Eval num_timesteps=16640, episode_reward=0.74 +/- 0.65\n",
      "Episode length: 30.12 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26640, episode_reward=0.75 +/- 0.64\n",
      "Episode length: 30.11 +/- 1.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36640, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=46640, episode_reward=0.71 +/- 0.68\n",
      "Episode length: 30.06 +/- 0.59\n",
      "Eval num_timesteps=56640, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 30.11 +/- 0.54\n",
      "Eval num_timesteps=66640, episode_reward=0.76 +/- 0.63\n",
      "Episode length: 29.96 +/- 2.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76640, episode_reward=0.73 +/- 0.67\n",
      "Episode length: 30.04 +/- 1.65\n",
      "Eval num_timesteps=86640, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.10 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96640, episode_reward=0.78 +/- 0.62\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=106640, episode_reward=0.73 +/- 0.66\n",
      "Episode length: 30.04 +/- 1.04\n",
      "Eval num_timesteps=116640, episode_reward=0.71 +/- 0.68\n",
      "Episode length: 30.08 +/- 1.17\n",
      "Eval num_timesteps=126640, episode_reward=0.71 +/- 0.68\n",
      "Episode length: 30.11 +/- 0.52\n",
      "Eval num_timesteps=136640, episode_reward=0.76 +/- 0.63\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=146640, episode_reward=0.83 +/- 0.53\n",
      "Episode length: 30.16 +/- 0.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156640, episode_reward=0.79 +/- 0.59\n",
      "Episode length: 30.12 +/- 0.52\n",
      "Eval num_timesteps=166640, episode_reward=0.74 +/- 0.65\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=176640, episode_reward=0.80 +/- 0.59\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=186640, episode_reward=0.78 +/- 0.61\n",
      "Episode length: 30.04 +/- 1.29\n",
      "Eval num_timesteps=196640, episode_reward=0.76 +/- 0.64\n",
      "Episode length: 30.03 +/- 1.29\n",
      "Eval num_timesteps=206640, episode_reward=0.82 +/- 0.56\n",
      "Episode length: 29.98 +/- 1.61\n",
      "Eval num_timesteps=216640, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.12 +/- 0.53\n",
      "Eval num_timesteps=226640, episode_reward=0.80 +/- 0.58\n",
      "Episode length: 30.06 +/- 1.17\n",
      "Eval num_timesteps=236640, episode_reward=0.80 +/- 0.58\n",
      "Episode length: 30.12 +/- 0.54\n",
      "Eval num_timesteps=246640, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=256640, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.08 +/- 0.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=266640, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=276640, episode_reward=0.76 +/- 0.64\n",
      "Episode length: 30.06 +/- 0.58\n",
      "Eval num_timesteps=286640, episode_reward=0.83 +/- 0.54\n",
      "Episode length: 30.16 +/- 0.54\n",
      "Eval num_timesteps=296640, episode_reward=0.80 +/- 0.57\n",
      "Episode length: 30.06 +/- 0.88\n",
      "Eval num_timesteps=306640, episode_reward=0.77 +/- 0.62\n",
      "Episode length: 29.98 +/- 1.47\n",
      "Eval num_timesteps=316640, episode_reward=0.85 +/- 0.52\n",
      "Episode length: 30.11 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=326640, episode_reward=0.84 +/- 0.52\n",
      "Episode length: 30.08 +/- 1.20\n",
      "Eval num_timesteps=336640, episode_reward=0.77 +/- 0.63\n",
      "Episode length: 30.05 +/- 1.13\n",
      "Eval num_timesteps=346640, episode_reward=0.83 +/- 0.55\n",
      "Episode length: 30.14 +/- 0.58\n",
      "Eval num_timesteps=356640, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.12 +/- 0.69\n",
      "Eval num_timesteps=366640, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.06 +/- 0.57\n",
      "Eval num_timesteps=376640, episode_reward=0.80 +/- 0.59\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=386640, episode_reward=0.85 +/- 0.52\n",
      "Episode length: 30.08 +/- 1.29\n",
      "Eval num_timesteps=396640, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 29.93 +/- 1.87\n",
      "Eval num_timesteps=406640, episode_reward=0.82 +/- 0.56\n",
      "Episode length: 30.13 +/- 0.57\n",
      "Eval num_timesteps=416640, episode_reward=0.84 +/- 0.52\n",
      "Episode length: 30.15 +/- 0.63\n",
      "Eval num_timesteps=426640, episode_reward=0.78 +/- 0.61\n",
      "Episode length: 30.04 +/- 1.13\n",
      "Eval num_timesteps=436640, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=446640, episode_reward=0.82 +/- 0.55\n",
      "Episode length: 30.13 +/- 0.57\n",
      "Eval num_timesteps=456640, episode_reward=0.82 +/- 0.55\n",
      "Episode length: 30.13 +/- 0.61\n",
      "Eval num_timesteps=466640, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=476640, episode_reward=0.82 +/- 0.54\n",
      "Episode length: 30.15 +/- 0.66\n",
      "Eval num_timesteps=486640, episode_reward=0.84 +/- 0.52\n",
      "Episode length: 30.12 +/- 0.52\n",
      "Eval num_timesteps=496640, episode_reward=0.83 +/- 0.55\n",
      "Episode length: 30.14 +/- 0.53\n",
      "Eval num_timesteps=506640, episode_reward=0.83 +/- 0.55\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=516640, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.06 +/- 1.19\n",
      "Eval num_timesteps=526640, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.04 +/- 1.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=536640, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=546640, episode_reward=0.88 +/- 0.46\n",
      "Episode length: 30.13 +/- 0.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=556640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.12 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=566640, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=576640, episode_reward=0.87 +/- 0.49\n",
      "Episode length: 30.09 +/- 1.26\n",
      "Eval num_timesteps=586640, episode_reward=0.86 +/- 0.48\n",
      "Episode length: 30.11 +/- 1.28\n",
      "Eval num_timesteps=596640, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.15 +/- 0.56\n",
      "Eval num_timesteps=606640, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.13 +/- 0.60\n",
      "Eval num_timesteps=616640, episode_reward=0.84 +/- 0.52\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=626640, episode_reward=0.86 +/- 0.50\n",
      "Episode length: 30.06 +/- 1.10\n",
      "Eval num_timesteps=636640, episode_reward=0.86 +/- 0.48\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=646640, episode_reward=0.86 +/- 0.49\n",
      "Episode length: 30.11 +/- 0.54\n",
      "Eval num_timesteps=656640, episode_reward=0.83 +/- 0.55\n",
      "Episode length: 30.07 +/- 0.61\n",
      "Eval num_timesteps=666640, episode_reward=0.83 +/- 0.55\n",
      "Episode length: 30.09 +/- 0.53\n",
      "Eval num_timesteps=676640, episode_reward=0.81 +/- 0.56\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=686640, episode_reward=0.86 +/- 0.51\n",
      "Episode length: 30.16 +/- 0.56\n",
      "Eval num_timesteps=696640, episode_reward=0.85 +/- 0.52\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=706640, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=716640, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.03 +/- 1.26\n",
      "Eval num_timesteps=726640, episode_reward=0.87 +/- 0.47\n",
      "Episode length: 30.11 +/- 0.60\n",
      "Eval num_timesteps=736640, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.13 +/- 0.57\n",
      "Eval num_timesteps=746640, episode_reward=0.85 +/- 0.50\n",
      "Episode length: 30.09 +/- 1.25\n",
      "Eval num_timesteps=756640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.16 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=766640, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=776640, episode_reward=0.82 +/- 0.55\n",
      "Episode length: 30.10 +/- 0.54\n",
      "Eval num_timesteps=786640, episode_reward=0.85 +/- 0.52\n",
      "Episode length: 30.09 +/- 0.58\n",
      "Eval num_timesteps=796640, episode_reward=0.86 +/- 0.49\n",
      "Episode length: 30.12 +/- 0.55\n",
      "Eval num_timesteps=806640, episode_reward=0.86 +/- 0.49\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=816640, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.08 +/- 0.55\n",
      "Eval num_timesteps=826640, episode_reward=0.86 +/- 0.50\n",
      "Episode length: 30.13 +/- 0.61\n",
      "Eval num_timesteps=836640, episode_reward=0.85 +/- 0.52\n",
      "Episode length: 30.07 +/- 0.59\n",
      "Eval num_timesteps=846640, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.09 +/- 1.24\n",
      "Eval num_timesteps=856640, episode_reward=0.87 +/- 0.49\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=866640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=876640, episode_reward=0.81 +/- 0.58\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=886640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.07 +/- 1.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=896640, episode_reward=0.85 +/- 0.50\n",
      "Episode length: 30.17 +/- 0.59\n",
      "Eval num_timesteps=906640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=916640, episode_reward=0.85 +/- 0.50\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=926640, episode_reward=0.89 +/- 0.45\n",
      "Episode length: 30.12 +/- 0.60\n",
      "Eval num_timesteps=936640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.13 +/- 0.60\n",
      "Eval num_timesteps=946640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=956640, episode_reward=0.86 +/- 0.51\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=966640, episode_reward=0.91 +/- 0.41\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=976640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.16 +/- 0.58\n",
      "Eval num_timesteps=986640, episode_reward=0.89 +/- 0.45\n",
      "Episode length: 30.03 +/- 1.28\n",
      "Eval num_timesteps=996640, episode_reward=0.84 +/- 0.54\n",
      "Episode length: 30.07 +/- 1.24\n",
      "Eval num_timesteps=1006640, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.05 +/- 1.25\n",
      "Eval num_timesteps=1016640, episode_reward=0.89 +/- 0.45\n",
      "Episode length: 30.08 +/- 1.27\n",
      "Eval num_timesteps=1026640, episode_reward=0.93 +/- 0.37\n",
      "Episode length: 30.09 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1036640, episode_reward=0.88 +/- 0.47\n",
      "Episode length: 30.13 +/- 0.61\n",
      "Eval num_timesteps=1046640, episode_reward=0.89 +/- 0.43\n",
      "Episode length: 30.12 +/- 0.54\n",
      "Eval num_timesteps=1056640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 29.98 +/- 1.55\n",
      "Eval num_timesteps=1066640, episode_reward=0.85 +/- 0.52\n",
      "Episode length: 30.14 +/- 0.60\n",
      "Eval num_timesteps=1076640, episode_reward=0.85 +/- 0.50\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=1086640, episode_reward=0.88 +/- 0.46\n",
      "Episode length: 30.11 +/- 0.56\n",
      "Eval num_timesteps=1096640, episode_reward=0.87 +/- 0.46\n",
      "Episode length: 30.09 +/- 0.66\n",
      "Eval num_timesteps=1106640, episode_reward=0.86 +/- 0.50\n",
      "Episode length: 30.09 +/- 0.81\n",
      "Eval num_timesteps=1116640, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.14 +/- 0.60\n",
      "Eval num_timesteps=1126640, episode_reward=0.86 +/- 0.50\n",
      "Episode length: 30.07 +/- 0.55\n",
      "Eval num_timesteps=1136640, episode_reward=0.87 +/- 0.47\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=1146640, episode_reward=0.88 +/- 0.47\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=1156640, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.09 +/- 0.76\n",
      "Eval num_timesteps=1166640, episode_reward=0.89 +/- 0.45\n",
      "Episode length: 30.12 +/- 1.31\n",
      "Eval num_timesteps=1176640, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.17 +/- 0.58\n",
      "Eval num_timesteps=1186640, episode_reward=0.88 +/- 0.45\n",
      "Episode length: 30.06 +/- 1.13\n",
      "Eval num_timesteps=1196640, episode_reward=0.88 +/- 0.47\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=1206640, episode_reward=0.86 +/- 0.49\n",
      "Episode length: 29.99 +/- 1.67\n",
      "Eval num_timesteps=1216640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=1226640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.18 +/- 0.58\n",
      "Eval num_timesteps=1236640, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=1246640, episode_reward=0.88 +/- 0.46\n",
      "Episode length: 30.10 +/- 1.14\n",
      "Eval num_timesteps=1256640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.12 +/- 0.61\n",
      "Eval num_timesteps=1266640, episode_reward=0.87 +/- 0.47\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=1276640, episode_reward=0.87 +/- 0.47\n",
      "Episode length: 30.11 +/- 1.24\n",
      "Eval num_timesteps=1286640, episode_reward=0.86 +/- 0.51\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=1296640, episode_reward=0.89 +/- 0.43\n",
      "Episode length: 30.12 +/- 0.54\n",
      "Eval num_timesteps=1306640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.15 +/- 0.57\n",
      "Eval num_timesteps=1316640, episode_reward=0.91 +/- 0.39\n",
      "Episode length: 30.15 +/- 0.56\n",
      "Eval num_timesteps=1326640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.11 +/- 0.51\n",
      "Eval num_timesteps=1336640, episode_reward=0.89 +/- 0.43\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=1346640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=1356640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=1366640, episode_reward=0.89 +/- 0.45\n",
      "Episode length: 30.17 +/- 0.57\n",
      "Eval num_timesteps=1376640, episode_reward=0.93 +/- 0.35\n",
      "Episode length: 30.18 +/- 0.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1386640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.18 +/- 0.57\n",
      "Eval num_timesteps=1396640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.09 +/- 1.03\n",
      "Eval num_timesteps=1406640, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.13 +/- 0.57\n",
      "Eval num_timesteps=1416640, episode_reward=0.92 +/- 0.37\n",
      "Episode length: 30.17 +/- 0.57\n",
      "Eval num_timesteps=1426640, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=1436640, episode_reward=0.93 +/- 0.37\n",
      "Episode length: 30.12 +/- 0.55\n",
      "Eval num_timesteps=1446640, episode_reward=0.93 +/- 0.35\n",
      "Episode length: 30.16 +/- 0.55\n",
      "Eval num_timesteps=1456640, episode_reward=0.95 +/- 0.31\n",
      "Episode length: 30.16 +/- 0.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1466640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=1476640, episode_reward=0.92 +/- 0.37\n",
      "Episode length: 30.14 +/- 0.54\n",
      "Eval num_timesteps=1486640, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.13 +/- 0.57\n",
      "Eval num_timesteps=1496640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.16 +/- 0.58\n",
      "Eval num_timesteps=1506640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.13 +/- 0.59\n",
      "Eval num_timesteps=1516640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.12 +/- 0.61\n",
      "Eval num_timesteps=1526640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=1536640, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.10 +/- 0.50\n",
      "Eval num_timesteps=1546640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.15 +/- 0.58\n",
      "Eval num_timesteps=1556640, episode_reward=0.93 +/- 0.34\n",
      "Episode length: 30.15 +/- 0.56\n",
      "Eval num_timesteps=1566640, episode_reward=0.94 +/- 0.32\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=1576640, episode_reward=0.89 +/- 0.43\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=1586640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=1596640, episode_reward=0.92 +/- 0.37\n",
      "Episode length: 30.12 +/- 0.60\n",
      "Eval num_timesteps=1606640, episode_reward=0.89 +/- 0.45\n",
      "Episode length: 30.18 +/- 0.57\n",
      "Eval num_timesteps=1616640, episode_reward=0.93 +/- 0.34\n",
      "Episode length: 30.17 +/- 0.58\n",
      "Eval num_timesteps=1626640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=1636640, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=1646640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=1656640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=1666640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.18 +/- 0.55\n",
      "Eval num_timesteps=1676640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.16 +/- 0.55\n",
      "Eval num_timesteps=1686640, episode_reward=0.87 +/- 0.47\n",
      "Episode length: 30.15 +/- 0.59\n",
      "Eval num_timesteps=1696640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.13 +/- 0.59\n",
      "Eval num_timesteps=1706640, episode_reward=0.90 +/- 0.41\n",
      "Episode length: 30.13 +/- 0.53\n",
      "Eval num_timesteps=1716640, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=1726640, episode_reward=0.91 +/- 0.41\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=1736640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.05 +/- 0.51\n",
      "Eval num_timesteps=1746640, episode_reward=0.93 +/- 0.35\n",
      "Episode length: 30.11 +/- 0.56\n",
      "Eval num_timesteps=1756640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.02 +/- 1.21\n",
      "Eval num_timesteps=1766640, episode_reward=0.91 +/- 0.39\n",
      "Episode length: 30.17 +/- 0.60\n",
      "Eval num_timesteps=1776640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=1786640, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.15 +/- 0.55\n",
      "Eval num_timesteps=1796640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.12 +/- 0.53\n",
      "Eval num_timesteps=1806640, episode_reward=0.91 +/- 0.39\n",
      "Episode length: 30.07 +/- 1.09\n",
      "Eval num_timesteps=1816640, episode_reward=0.91 +/- 0.41\n",
      "Episode length: 30.15 +/- 0.57\n",
      "Eval num_timesteps=1826640, episode_reward=0.90 +/- 0.40\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=1836640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.15 +/- 0.59\n",
      "Eval num_timesteps=1846640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=1856640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.14 +/- 0.54\n",
      "Eval num_timesteps=1866640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=1876640, episode_reward=0.94 +/- 0.32\n",
      "Episode length: 30.16 +/- 0.58\n",
      "Eval num_timesteps=1886640, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.02 +/- 1.60\n",
      "Eval num_timesteps=1896640, episode_reward=0.90 +/- 0.44\n",
      "Episode length: 30.05 +/- 1.29\n",
      "Eval num_timesteps=1906640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=1916640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=1926640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.10 +/- 1.15\n",
      "Eval num_timesteps=1936640, episode_reward=0.91 +/- 0.42\n",
      "Episode length: 30.14 +/- 0.59\n",
      "Eval num_timesteps=1946640, episode_reward=0.92 +/- 0.40\n",
      "Episode length: 30.13 +/- 0.64\n",
      "Eval num_timesteps=1956640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.15 +/- 0.59\n",
      "Eval num_timesteps=1966640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.14 +/- 0.60\n",
      "Eval num_timesteps=1976640, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.08 +/- 0.93\n",
      "Eval num_timesteps=1986640, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.10 +/- 0.60\n",
      "Eval num_timesteps=1996640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.09 +/- 0.60\n",
      "Eval num_timesteps=2006640, episode_reward=0.92 +/- 0.37\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=2016640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.13 +/- 0.55\n",
      "Eval num_timesteps=2026640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.14 +/- 0.63\n",
      "Eval num_timesteps=2036640, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=2046640, episode_reward=0.91 +/- 0.38\n",
      "Episode length: 30.11 +/- 1.17\n",
      "Eval num_timesteps=2056640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=2066640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=2076640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.04 +/- 1.32\n",
      "Eval num_timesteps=2086640, episode_reward=0.86 +/- 0.49\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=2096640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.01 +/- 1.67\n",
      "Eval num_timesteps=2106640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.10 +/- 0.57\n",
      "Eval num_timesteps=2116640, episode_reward=0.93 +/- 0.35\n",
      "Episode length: 30.11 +/- 0.49\n",
      "Eval num_timesteps=2126640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.10 +/- 1.28\n",
      "Eval num_timesteps=2136640, episode_reward=0.94 +/- 0.32\n",
      "Episode length: 30.07 +/- 1.29\n",
      "Eval num_timesteps=2146640, episode_reward=0.91 +/- 0.39\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=2156640, episode_reward=0.93 +/- 0.35\n",
      "Episode length: 30.12 +/- 1.17\n",
      "Eval num_timesteps=2166640, episode_reward=0.93 +/- 0.35\n",
      "Episode length: 30.12 +/- 0.68\n",
      "Eval num_timesteps=2176640, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=2186640, episode_reward=0.94 +/- 0.31\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=2196640, episode_reward=0.93 +/- 0.34\n",
      "Episode length: 30.13 +/- 0.59\n",
      "Eval num_timesteps=2206640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=2216640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.15 +/- 0.61\n",
      "Eval num_timesteps=2226640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=2236640, episode_reward=0.93 +/- 0.37\n",
      "Episode length: 30.12 +/- 1.18\n",
      "Eval num_timesteps=2246640, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=2256640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.10 +/- 0.54\n",
      "Eval num_timesteps=2266640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.11 +/- 0.60\n",
      "Eval num_timesteps=2276640, episode_reward=0.94 +/- 0.32\n",
      "Episode length: 30.17 +/- 0.60\n",
      "Eval num_timesteps=2286640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=2296640, episode_reward=0.95 +/- 0.30\n",
      "Episode length: 30.13 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2306640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.14 +/- 0.55\n",
      "Eval num_timesteps=2316640, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.07 +/- 0.58\n",
      "Eval num_timesteps=2326640, episode_reward=0.95 +/- 0.29\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=2336640, episode_reward=0.93 +/- 0.37\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=2346640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.16 +/- 0.61\n",
      "Eval num_timesteps=2356640, episode_reward=0.93 +/- 0.34\n",
      "Episode length: 30.14 +/- 0.55\n",
      "Eval num_timesteps=2366640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.05 +/- 1.29\n",
      "Eval num_timesteps=2376640, episode_reward=0.94 +/- 0.32\n",
      "Episode length: 30.03 +/- 1.25\n",
      "Eval num_timesteps=2386640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.08 +/- 0.54\n",
      "Eval num_timesteps=2396640, episode_reward=0.94 +/- 0.33\n",
      "Episode length: 30.18 +/- 0.60\n",
      "Eval num_timesteps=2406640, episode_reward=0.92 +/- 0.38\n",
      "Episode length: 30.11 +/- 0.54\n",
      "Eval num_timesteps=2416640, episode_reward=0.91 +/- 0.42\n",
      "Episode length: 30.06 +/- 1.31\n",
      "Eval num_timesteps=2426640, episode_reward=0.96 +/- 0.27\n",
      "Episode length: 30.13 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2436640, episode_reward=0.93 +/- 0.34\n",
      "Episode length: 30.16 +/- 0.58\n",
      "Eval num_timesteps=2446640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.10 +/- 0.61\n",
      "Eval num_timesteps=2456640, episode_reward=0.93 +/- 0.37\n",
      "Episode length: 30.13 +/- 0.59\n",
      "Eval num_timesteps=2466640, episode_reward=0.94 +/- 0.33\n",
      "Episode length: 30.13 +/- 0.57\n",
      "Eval num_timesteps=2476640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.16 +/- 0.84\n",
      "Eval num_timesteps=2486640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=2496640, episode_reward=0.91 +/- 0.39\n",
      "Episode length: 30.09 +/- 1.30\n",
      "Eval num_timesteps=2506640, episode_reward=0.94 +/- 0.35\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=2516640, episode_reward=0.93 +/- 0.34\n",
      "Episode length: 30.17 +/- 0.58\n",
      "Eval num_timesteps=2526640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.12 +/- 0.55\n",
      "Eval num_timesteps=2536640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.17 +/- 0.59\n",
      "Eval num_timesteps=2546640, episode_reward=0.94 +/- 0.32\n",
      "Episode length: 30.16 +/- 0.57\n",
      "Eval num_timesteps=2556640, episode_reward=0.93 +/- 0.35\n",
      "Episode length: 30.07 +/- 1.56\n",
      "Eval num_timesteps=2566640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.12 +/- 0.55\n",
      "Eval num_timesteps=2576640, episode_reward=0.93 +/- 0.36\n",
      "Episode length: 30.08 +/- 1.31\n",
      "Eval num_timesteps=2586640, episode_reward=0.92 +/- 0.36\n",
      "Episode length: 29.98 +/- 1.83\n",
      "Eval num_timesteps=2596640, episode_reward=0.90 +/- 0.41\n",
      "Episode length: 30.05 +/- 1.21\n",
      "Eval num_timesteps=2606640, episode_reward=0.91 +/- 0.39\n",
      "Episode length: 30.15 +/- 0.61\n",
      "Eval num_timesteps=2616640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.11 +/- 0.54\n",
      "Eval num_timesteps=2626640, episode_reward=0.94 +/- 0.32\n",
      "Episode length: 30.06 +/- 1.21\n",
      "Eval num_timesteps=2636640, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.10 +/- 0.61\n",
      "Eval num_timesteps=2646640, episode_reward=0.92 +/- 0.37\n",
      "Episode length: 30.11 +/- 0.59\n",
      "Eval num_timesteps=2656640, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=2666640, episode_reward=0.93 +/- 0.34\n",
      "Episode length: 30.16 +/- 0.55\n",
      "Eval num_timesteps=2676640, episode_reward=0.92 +/- 0.36\n",
      "Episode length: 30.09 +/- 0.53\n",
      "Eval num_timesteps=2686640, episode_reward=0.94 +/- 0.32\n",
      "Episode length: 30.12 +/- 0.55\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e10), callback=[eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626a9c5-67c7-419f-aee9-4c2ea6a6ba6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
